import logging
import os
import io
import asyncio
import wave
import base64
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import google.generativeai as genai
from dotenv import load_dotenv
from pydub import AudioSegment

# -----------------------------
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù…Ù† .env
# -----------------------------
load_dotenv()
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­
if not TELEGRAM_BOT_TOKEN:
    logging.error("âŒ TELEGRAM_BOT_TOKEN is not set.")
    raise ValueError("âŒ ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¶Ø¹ TELEGRAM_BOT_TOKEN ÙÙŠ Ù…Ù„Ù .env")

if not GEMINI_API_KEY:
    logging.error("âŒ GEMINI_API_KEY is not set.")
    raise ValueError("âŒ ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¶Ø¹ GEMINI_API_KEY ÙÙŠ Ù…Ù„Ù .env")

# -----------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Logging
# -----------------------------
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# -----------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Gemini
# -----------------------------
genai.configure(api_key=GEMINI_API_KEY)

text_model = genai.GenerativeModel("gemini-1.5-flash")
tts_model = genai.GenerativeModel(model_name="gemini-2.5-flash-preview-tts")
image_model = genai.GenerativeModel(model_name="gemini-2.0-flash-preview-image-generation")
transcription_model = genai.GenerativeModel("gemini-1.5-pro-latest")

# -----------------------------
# Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ
# -----------------------------
PROFESSIONAL_PROMPT = """
Ø£Ù†Øª Ø§Ù„Ø¢Ù† ØªØ¹Ù…Ù„ ÙƒØ¨ÙˆØª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù„ØºØ§ÙŠØ©ØŒ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§ØªØŒ ÙˆÙˆØ¯ÙŠØŒ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©.
Ù‡Ø¯ÙÙƒ Ù‡Ùˆ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ù…ÙƒÙ†Ø©ØŒ Ù…Ø¹ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.
"""

# -----------------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨ÙˆØª
# -----------------------------
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ ÙƒØ¬Ø²Ø¡ Ù…Ù† ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    context.user_data['chat_history'] = [{"role": "user", "parts": [{"text": PROFESSIONAL_PROMPT}]}]
    await update.message.reply_text(
        "ğŸ‘‹ Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Gemini).\n"
        "Ø£Ø±Ø³Ù„ Ù„ÙŠ Ø£ÙŠ Ø±Ø³Ø§Ù„Ø© Ù†ØµÙŠØ© Ø£Ùˆ ØµÙˆØªÙŠØ© ÙˆØ³Ø£Ø±Ø¯ Ø¹Ù„ÙŠÙƒ.\n"
        "ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø± /image Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø£Ùˆ /clear Ù„Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©."
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "â„¹ï¸ Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø¨ÙˆØª:\n"
        "/start - Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª\n"
        "/help - Ø¹Ø±Ø¶ Ø§Ù„Ø£ÙˆØ§Ù…Ø±\n"
        "/image [ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø©] - Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n"
        "/clear - Ù„Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"
    )

async def clear_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data['chat_history'] = []
    await update.message.reply_text("âœ… ØªÙ… Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.")

# -----------------------------
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª WAV
# -----------------------------
async def convert_text_to_wav(text: str) -> io.BytesIO:
    try:
        generation_config = genai.types.GenerationConfig(
            speech_config=genai.types.SpeechConfig(
                voice_config=genai.types.VoiceConfig(
                    prebuilt_voice_config=genai.types.PrebuiltVoiceConfig(
                        voice_name="Kore"
                    )
                )
            )
        )
        response = await asyncio.to_thread(
            tts_model.generate_content,
            contents=[{"parts": [{"text": text}]}],
            generation_config=generation_config
        )
        if not response.candidates:
            logger.error("âŒ Gemini TTS returned no candidates")
            return None
        
        audio_part = next((p for p in response.candidates[0].content.parts if "inlineData" in p), None)
        if not audio_part:
            logger.error("âŒ Gemini TTS returned no audio data")
            return None

        audio_data = base64.b64decode(audio_part.inlineData.data)
        audio_segment = AudioSegment(
            data=audio_data,
            sample_width=2,
            frame_rate=24000,
            channels=1
        )
        audio_stream = io.BytesIO()
        audio_segment.export(audio_stream, format="wav")
        audio_stream.seek(0)
        return audio_stream
    except Exception as e:
        logger.error(f"âŒ TTS error: {e}")
        return None

# -----------------------------
# ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±
# -----------------------------
async def _generate_and_send_image(update: Update, context: ContextTypes.DEFAULT_TYPE, prompt: str):
    if not prompt:
        await update.message.reply_text("ğŸ¤” Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø±Ø³Ù„ ÙˆØµÙÙ‹Ø§ Ù„Ù„ØµÙˆØ±Ø©.")
        return
    try:
        await update.message.reply_text("â³ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©...")
        full_prompt = f"generate a creative and imaginative image: {prompt}"
        image_response = await asyncio.to_thread(
            image_model.generate_content,
            contents=[{"parts": [{"text": full_prompt}]}],
            response_modalities=["IMAGE", "TEXT"]
        )
        if not image_response.candidates:
            await update.message.reply_text("âŒ Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©.")
            return

        image_part = next((p for p in image_response.candidates[0].content.parts if "inlineData" in p), None)
        if not image_part:
            await update.message.reply_text("âŒ Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.")
            return
        
        image_data = base64.b64decode(image_part.inlineData.data)
        await update.message.reply_photo(photo=io.BytesIO(image_data), caption="âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©!")
    except Exception as e:
        logger.error(f"âŒ Image generation error: {e}")
        await update.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©.")

async def handle_image_generation(update: Update, context: ContextTypes.DEFAULT_TYPE):
    prompt = " ".join(context.args)
    await _generate_and_send_image(update, context, prompt)

# -----------------------------
# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
# -----------------------------
async def process_text_and_respond(update: Update, context: ContextTypes.DEFAULT_TYPE, user_text: str):
    try:
        # Initialize chat_history if it doesn't exist
        if 'chat_history' not in context.user_data:
            context.user_data['chat_history'] = [{"role": "user", "parts": [{"text": PROFESSIONAL_PROMPT}]}]
        
        chat_history = context.user_data['chat_history']
        
        # Start a new chat session with the current history
        chat_session = text_model.start_chat(history=chat_history)
        
        # Send the user's message to the model
        text_response = await asyncio.to_thread(chat_session.send_message, user_text)
        
        if text_response.candidates and text_response.candidates[0].content.parts:
            bot_reply = text_response.candidates[0].content.parts[0].text
            # Append the user and model messages to the history for context
            chat_history.append({"role": "user", "parts": [{"text": user_text}]})
            chat_history.append({"role": "model", "parts": [{"text": bot_reply}]})
        else:
            bot_reply = "ğŸ¤– Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨."

        await update.message.reply_text(bot_reply)
        audio_stream = await convert_text_to_wav(bot_reply)
        if audio_stream:
            await update.message.reply_voice(voice=audio_stream)
    except Exception as e:
        logger.error(f"âŒ Text processing error: {e}")
        await update.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ.")

async def handle_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_message = update.message.text
    image_keywords = ["Ø§Ù†Ø´Ø¦ ØµÙˆØ±Ø©", "Ø§Ø±Ø³Ù…", "ØµÙˆØ±Ø©", "ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø©", "Ø§Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø©"]
    for keyword in image_keywords:
        if keyword in user_message:
            prompt = user_message.replace(keyword, "", 1).strip()
            await _generate_and_send_image(update, context, prompt)
            return
    await process_text_and_respond(update, context, user_message)

# -----------------------------
# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ØµÙˆØªÙŠØ©
# -----------------------------
async def handle_voice_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        processing_message = await update.message.reply_text("â³ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†Øµ...")
        
        file_id = update.message.voice.file_id
        voice_file = await context.bot.get_file(file_id)
        voice_data = await voice_file.download_as_bytearray()
        voice_stream = io.BytesIO(voice_data)
        
        audio_segment = AudioSegment.from_file(voice_stream, format="ogg")
        wav_stream = io.BytesIO()
        audio_segment.export(wav_stream, format="wav")
        wav_stream.seek(0)
        
        audio_part = {
            "mime_type": "audio/wav",
            "data": base64.b64encode(wav_stream.read()).decode('utf-8')
        }
        
        transcription_prompt = [{"audio": audio_part}]
        transcription_response = await asyncio.to_thread(transcription_model.generate_content, transcription_prompt)
        
        if transcription_response.candidates and transcription_response.candidates[0].content.parts:
            transcribed_text = transcription_response.candidates[0].content.parts[0].text
            logger.info(f"âœ… Transcribed text: {transcribed_text}")
            
            await context.bot.edit_message_text(
                chat_id=processing_message.chat_id,
                message_id=processing_message.message_id,
                text=f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {transcribed_text}"
            )
            
            await process_text_and_respond(update, context, transcribed_text)
        else:
            await context.bot.edit_message_text(
                chat_id=processing_message.chat_id,
                message_id=processing_message.message_id,
                text="âŒ Ù„Ù… Ø£Ø³ØªØ·Ø¹ ÙÙ‡Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØµÙˆØªÙŠØ©."
            )
            
    except Exception as e:
        logger.error(f"âŒ Voice processing error: {e}")
        await update.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØµÙˆØªÙŠØ©.")

# -----------------------------
# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª
# -----------------------------
def main():
    try:
        app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
        app.add_handler(CommandHandler("start", start))
        app.add_handler(CommandHandler("help", help_command))
        app.add_handler(CommandHandler("clear", clear_command))
        app.add_handler(CommandHandler("image", handle_image_generation))
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text_message))
        app.add_handler(MessageHandler(filters.VOICE, handle_voice_message))
        logger.info("âœ… Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù† ...")
        app.run_polling()
    except Exception as e:
        logger.error(f"âŒ Bot startup error: {e}")

if __name__ == "__main__":
    main()